## understanding_backpropagation
This project provides an educational implementation of an automatic differentiation engine inspired by Micrograd. It includes the creation and training of a basic neural network from scratch, aiming to deepen the understanding of backpropagation and gradient computation.

#The notebook includes sections that cover:

Defining and plotting a simple function - Basic function plotting for intuitive understanding.
Numerical derivative approximation - Approximating derivatives to grasp gradient concepts.
Implementing the Value class - Building the foundation for automatic differentiation.
Visualizing the computational graph - Using graphviz to visualize the operations.
Neural network implementation - Creating a simple neural network (MLP) from scratch.
Gradient descent with PyTorch - Comparing custom implementation results with PyTorch.

For questions or suggestions, feel free to contact:

Pragay - kumarpragay@gmail.com
